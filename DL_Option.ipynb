{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPYpf7wx+kUlpx5fdT6XeN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hB_kdjPJW0k",
        "outputId": "33dbfd0f-5f36-4e8c-b9d3-4e7dc2df3004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcuXSOFNVMEh"
      },
      "outputs": [],
      "source": [
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "cupy.cuda.set_allocator(None)\n",
        "from torch.utils.dlpack import from_dlpack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "extern \"C\" __global__ void batched_barrier_option(\n",
        "    float *d_s,\n",
        "    const float T,\n",
        "    const float * K,\n",
        "    const float * B,\n",
        "    const float * S0,\n",
        "    const float * sigma,\n",
        "    const float * mu,\n",
        "    const float * r,\n",
        "    const float * d_normals,\n",
        "    const long N_STEPS,\n",
        "    const long N_PATHS,\n",
        "    const long N_BATCH)\n",
        "{\n",
        "  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  unsigned stride = blockDim.x * gridDim.x;\n",
        "  unsigned tid = threadIdx.x;\n",
        "  const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "  for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "  {\n",
        "    int batch_id = i / N_PATHS;\n",
        "    int path_id = i % N_PATHS;\n",
        "    float s_curr = S0[batch_id];\n",
        "    float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "    float tmp2 = exp(-r[batch_id]*T);\n",
        "    unsigned n=0;\n",
        "    double running_average = 0.0;\n",
        "    for(unsigned n = 0; n < N_STEPS; n++){\n",
        "       s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "       running_average += (s_curr - running_average) / (n + 1.0);\n",
        "       if (running_average <= B[batch_id]){\n",
        "           break;\n",
        "       }\n",
        "    }\n",
        "\n",
        "    float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f);\n",
        "    d_s[i] = tmp2 * payoff;\n",
        "  }\n",
        "}\n",
        "\n",
        "''', 'batched_barrier_option')"
      ],
      "metadata": {
        "id": "0lREUNbfAptj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_PATHS = 2048000\n",
        "N_STEPS = 365\n",
        "N_BATCH = 2\n",
        "T = 1.0\n",
        "\n",
        "K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
        "B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
        "S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
        "sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
        "mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
        "r =cupy.array([0.05, 0.05], dtype=cupy.float32)\n"
      ],
      "metadata": {
        "id": "sgOxEBhNCFGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_run():\n",
        "    number_of_threads = 256\n",
        "    number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
        "    randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
        "    output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
        "    cupy.cuda.stream.get_current_stream().synchronize()\n",
        "    s = time.time()\n",
        "    cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
        "                       (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
        "                        randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
        "    v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
        "    cupy.cuda.stream.get_current_stream().synchronize()\n",
        "    e = time.time()\n",
        "    print('time', e-s, 'v',v)\n",
        "batch_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GOIzLOVCIYs",
        "outputId": "d7084453-5f80-488f-b8b7-c0cbe70393ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time 0.49608469009399414 v [21.20576    0.8466831]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/asian_barrier_option/cupy_dataset.py', 'w') as f:\n",
        "    f.write(\"\"\")\n",
        "import cupy\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "cupy.cuda.set_allocator(None)\n",
        "cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
        "extern \"C\" __global__ void batched_barrier_option(\n",
        "    float *d_s,\n",
        "    const float T,\n",
        "    const float * K,\n",
        "    const float * B,\n",
        "    const float * S0,\n",
        "    const float * sigma,\n",
        "    const float * mu,\n",
        "    const float * r,\n",
        "    const float * d_normals,\n",
        "    const long N_STEPS,\n",
        "    const long N_PATHS,\n",
        "    const long N_BATCH)\n",
        "{\n",
        "  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  unsigned stride = blockDim.x * gridDim.x;\n",
        "  unsigned tid = threadIdx.x;\n",
        "  const float tmp3 = sqrt(T/N_STEPS);\n",
        "\n",
        "\n",
        "  for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
        "  {\n",
        "    int batch_id = i / N_PATHS;\n",
        "    int path_id = i % N_PATHS;\n",
        "    float s_curr = S0[batch_id];\n",
        "    float tmp1 = mu[batch_id]*T/N_STEPS;\n",
        "    float tmp2 = exp(-r[batch_id]*T);\n",
        "    unsigned n=0;\n",
        "    double running_average = 0.0;\n",
        "    for(unsigned n = 0; n < N_STEPS; n++){\n",
        "       s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
        "       running_average += (s_curr - running_average) / (n + 1.0);\n",
        "       if (running_average <= B[batch_id]){\n",
        "           break;\n",
        "       }\n",
        "    }\n",
        "\n",
        "    float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f);\n",
        "    d_s[i] = tmp2 * payoff;\n",
        "  }\n",
        "}\n",
        "\n",
        "''', 'batched_barrier_option')\n",
        "class OptionDataSet(torch.utils.data.IterableDataset):\n",
        "\n",
        "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
        "        self.num = 0\n",
        "        self.max_length = max_len\n",
        "        self.N_PATHS = number_path\n",
        "        self.N_STEPS = 365\n",
        "        self.N_BATCH = batch\n",
        "        self.T = np.float32(1.0)\n",
        "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32)\n",
        "        self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
        "        self.number_of_threads = threads\n",
        "        cupy.random.seed(seed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_length\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.num = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.num > self.max_length:\n",
        "            raise StopIteration\n",
        "        X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
        "        # scale the [0, 1) random numbers to the correct range for each of the option parameters\n",
        "        X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
        "        # make sure the Barrier is smaller than the Strike price\n",
        "        X[:, 1] = X[:, 0] * X[:, 1]\n",
        "        randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
        "        cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]),\n",
        "                              cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
        "        Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
        "        self.num += 1\n",
        "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "hrKOJjc-CTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = OptionDataSet(10, number_path=100000, batch=16, seed=15)\n",
        "for i in ds:\n",
        "    print(i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjguMJ2pC0Gb",
        "outputId": "846f97a5-c788-4500-f98d-caff52f5a4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.6558e+02, 0.0000e+00, 8.0069e+01, 1.0866e+02, 7.7740e-03, 0.0000e+00,\n",
            "        2.7772e+01, 0.0000e+00, 0.0000e+00, 6.4279e+01, 0.0000e+00, 5.1346e+00,\n",
            "        0.0000e+00, 1.4733e+02, 4.1851e+01, 0.0000e+00], device='cuda:0')\n",
            "tensor([ 57.1285,   0.0000,   0.0000, 151.9433,   0.0000,   0.0000,   0.0000,\n",
            "          9.3306,   0.0000,   0.7246, 157.0885,  10.7096,   0.0000,   0.7067,\n",
            "         59.1110,  14.6442], device='cuda:0')\n",
            "tensor([106.4531,   0.0000,  51.1248,  12.7823,  67.4821,   0.0000,   7.3539,\n",
            "          0.0000, 143.2203,  66.0655,  66.5476, 129.6811,   0.0000,  13.5559,\n",
            "         27.5546,   0.0000], device='cuda:0')\n",
            "tensor([4.1777e+01, 0.0000e+00, 2.5890e+00, 1.4500e+02, 0.0000e+00, 1.5099e+00,\n",
            "        1.1183e+02, 5.6967e+01, 7.5750e-05, 1.2390e+01, 0.0000e+00, 3.0183e+01,\n",
            "        1.3890e+01, 5.0533e+01, 3.8499e+01, 8.2232e+01], device='cuda:0')\n",
            "tensor([1.0687e+02, 3.0590e+01, 8.5428e+01, 1.9835e+01, 3.0602e+01, 1.5230e+00,\n",
            "        0.0000e+00, 0.0000e+00, 4.0244e+01, 0.0000e+00, 3.7487e-01, 0.0000e+00,\n",
            "        1.1777e+02, 0.0000e+00, 9.6200e+00, 4.2073e-04], device='cuda:0')\n",
            "tensor([ 83.6088, 125.8481,   0.0000,   0.0000,   0.0000,  35.1237,  26.4887,\n",
            "        114.6908,   1.2338, 133.6484,  84.3443,  49.0380,  33.3620,  93.0905,\n",
            "         40.8572,  30.2684], device='cuda:0')\n",
            "tensor([1.6068e+01, 6.8251e+01, 1.7516e+00, 6.3889e+01, 2.0682e+00, 3.0282e-01,\n",
            "        2.3074e-04, 2.4942e+01, 1.1639e+02, 0.0000e+00, 3.0597e+01, 0.0000e+00,\n",
            "        3.0390e+01, 2.1144e+00, 8.2769e-04, 6.3105e+01], device='cuda:0')\n",
            "tensor([129.0360,   0.0000,   0.0000,  34.7129,  76.3240,  61.5014,  96.1047,\n",
            "         41.5991,   0.0000,   0.0000,   1.6868,   0.0000,   0.0000, 198.8765,\n",
            "          0.0000, 130.8935], device='cuda:0')\n",
            "tensor([23.4824, 49.1953, 70.5731,  0.0000,  0.0000, 35.5231,  0.0000,  0.0000,\n",
            "         0.0000, 64.7129,  0.0000, 56.6821,  3.6377,  0.0000,  0.0000, 17.6415],\n",
            "       device='cuda:0')\n",
            "tensor([113.4123,   0.2840,   0.0000,   9.8790,  34.9789,  62.0461,   0.0000,\n",
            "          0.0000,  90.4281, 151.8807,   0.0000,   0.0000,  75.6426, 137.9153,\n",
            "          0.0000,  65.4237], device='cuda:0')\n",
            "tensor([1.1853e+02, 0.0000e+00, 0.0000e+00, 3.5182e+01, 8.2466e+01, 0.0000e+00,\n",
            "        0.0000e+00, 1.7089e+01, 0.0000e+00, 2.8777e-02, 0.0000e+00, 0.0000e+00,\n",
            "        6.7766e+01, 3.9360e+01, 1.2019e+02, 1.0623e+02], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/asian_barrier_option/model.py', 'w') as f:\n",
        "    f.write('''\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden=1024):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, hidden)\n",
        "        self.fc5 = nn.Linear(hidden, hidden)\n",
        "        self.fc6 = nn.Linear(hidden, 1)\n",
        "        self.register_buffer('norm',\n",
        "                             torch.tensor([200.0,\n",
        "                                           198.0,\n",
        "                                           200.0,\n",
        "                                           0.4,\n",
        "                                           0.2,\n",
        "                                           0.2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize the parameter to range [0-1]\n",
        "        x = x / self.norm\n",
        "        x = F.elu(self.fc1(x))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.elu(self.fc4(x))\n",
        "        x = F.elu(self.fc5(x))\n",
        "        return self.fc6(x)\n",
        "''')"
      ],
      "metadata": {
        "id": "bZPJhR72DOyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from ignite.handlers import Timer\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "timer = Timer(average=True)\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# set the AMP optimization level to O1\n",
        "opt_level = 'O1'\n",
        "# wrap the optimizer and model\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    # amp handles the auto loss scaling\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "timer.attach(trainer,\n",
        "             start=Events.EPOCH_STARTED,\n",
        "             resume=Events.ITERATION_STARTED,\n",
        "             pause=Events.ITERATION_COMPLETED,\n",
        "             step=Events.ITERATION_COMPLETED)\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "\n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output, 'average time', timer.value())\n",
        "\n",
        "trainer.run(dataset, max_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk92BqI6Qdi0",
        "outputId": "aec59bc6-2f5a-47d4-d28e-5da2def85f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
            "loss 18.9493465423584 average time 0.013191647169996941\n",
            "loss 3.8684794902801514 average time 0.01117357063998952\n",
            "loss 2.8061115741729736 average time 0.010493785163328842\n",
            "loss 1.9397087097167969 average time 0.010159658207498978\n",
            "loss 1.5167996883392334 average time 0.00994660571600798\n",
            "loss 1.2774827480316162 average time 0.009826248418342705\n",
            "loss 1.1729235649108887 average time 0.009742591385723115\n",
            "loss 1.0800435543060303 average time 0.009664763562514054\n",
            "loss 1.0796104669570923 average time 0.009607297745561584\n",
            "loss 0.9317582249641418 average time 0.009560219770010918\n",
            "loss 0.8657004833221436 average time 0.009529317457280823\n",
            "loss 0.8132827877998352 average time 0.00949539845750299\n",
            "loss 0.74169921875 average time 0.009475191746925283\n",
            "loss 0.7481200098991394 average time 0.00945327582857057\n",
            "loss 0.7731989622116089 average time 0.00943358075333102\n",
            "loss 0.6909382939338684 average time 0.00941829069999642\n",
            "loss 0.6259704828262329 average time 0.009394425324111687\n",
            "loss 0.6914814710617065 average time 0.00938368215554874\n",
            "loss 0.9329368472099304 average time 0.009383266595256684\n",
            "loss 0.5185776948928833 average time 0.009366828815996087\n",
            "loss 0.4979502856731415 average time 0.009357684085707661\n",
            "loss 0.5089589357376099 average time 0.009341947000900125\n",
            "loss 0.4500855505466461 average time 0.00933332776521045\n",
            "loss 0.45402491092681885 average time 0.009328800650409903\n",
            "loss 0.5207803249359131 average time 0.009316963165992637\n",
            "loss 0.38310161232948303 average time 0.009307642409224243\n",
            "loss 0.5303528904914856 average time 0.009298777692214976\n",
            "loss 0.3650311231613159 average time 0.009289693622134955\n",
            "loss 0.3218357563018799 average time 0.009290817983091356\n",
            "loss 0.4950014352798462 average time 0.009284212337320545\n",
            "loss 0.8728528022766113 average time 0.009275557817084137\n",
            "loss 0.42701631784439087 average time 0.009272339965299067\n",
            "loss 0.36503347754478455 average time 0.009265749640289318\n",
            "loss 0.387287437915802 average time 0.00926315858145619\n",
            "loss 0.3188040256500244 average time 0.009255934832557614\n",
            "loss 0.34867197275161743 average time 0.009248918336098137\n",
            "loss 0.3562011122703552 average time 0.009245831796475564\n",
            "loss 0.3148232102394104 average time 0.009240845613147412\n",
            "loss 0.361041396856308 average time 0.009237241702041548\n",
            "loss 0.34090566635131836 average time 0.009235706172239589\n",
            "loss 0.6126001477241516 average time 0.009231003644136298\n",
            "loss 0.41280263662338257 average time 0.009230130846180622\n",
            "loss 0.38277649879455566 average time 0.009228144334875376\n",
            "loss 0.318547785282135 average time 0.00922358242385516\n",
            "loss 0.37851426005363464 average time 0.009221538839992112\n",
            "loss 0.3764627277851105 average time 0.009216731979339112\n",
            "loss 0.2995529770851135 average time 0.00921726618084207\n",
            "loss 0.30662602186203003 average time 0.00921569684457457\n",
            "loss 0.35327568650245667 average time 0.009212135091624336\n",
            "loss 0.33178019523620605 average time 0.009209850082391495\n",
            "loss 0.3338298201560974 average time 0.009206917370384853\n",
            "loss 0.33261242508888245 average time 0.009206584513839027\n",
            "loss 0.3567761778831482 average time 0.009207072847919094\n",
            "loss 0.3272104561328888 average time 0.009204763813885351\n",
            "loss 0.3472989797592163 average time 0.009203891578177883\n",
            "loss 0.277370423078537 average time 0.009201478804638848\n",
            "loss 0.3229827582836151 average time 0.009199112864911225\n",
            "loss 0.31681686639785767 average time 0.009195794096378825\n",
            "loss 0.3164904713630676 average time 0.009192835785083767\n",
            "loss 0.40434351563453674 average time 0.009191583870165231\n",
            "loss 0.2514348030090332 average time 0.009190774360488787\n",
            "loss 0.2803869843482971 average time 0.009188770564029372\n",
            "loss 0.33982741832733154 average time 0.00918973100349138\n",
            "loss 0.32198095321655273 average time 0.009187369812343605\n",
            "loss 0.3122847080230713 average time 0.009184596250923698\n",
            "loss 0.2887338697910309 average time 0.009183348278333914\n",
            "loss 0.3013114035129547 average time 0.009181991156119014\n",
            "loss 0.3002549707889557 average time 0.009180557382351521\n",
            "loss 0.2873589098453522 average time 0.009178916262173542\n",
            "loss 0.2826811373233795 average time 0.009178118077571656\n",
            "loss 0.3025245666503906 average time 0.009176734639578415\n",
            "loss 0.32212308049201965 average time 0.009174646426668763\n",
            "loss 0.3140956163406372 average time 0.009171992012057697\n",
            "loss 0.29451078176498413 average time 0.009170583724191994\n",
            "loss 0.2984727621078491 average time 0.009169665401734528\n",
            "loss 0.32866495847702026 average time 0.009168948115264022\n",
            "loss 0.2945021688938141 average time 0.009167292340132374\n",
            "loss 0.3160538673400879 average time 0.009170751490516918\n",
            "loss 0.2716301679611206 average time 0.009173303755447889\n",
            "loss 0.36816513538360596 average time 0.009172036482003477\n",
            "loss 0.28449851274490356 average time 0.009170366625807847\n",
            "loss 0.3257196247577667 average time 0.009171084934030039\n",
            "loss 0.2929251790046692 average time 0.00916896617590929\n",
            "loss 0.28119707107543945 average time 0.009166685234651075\n",
            "loss 0.26723650097846985 average time 0.00916688805330234\n",
            "loss 0.32208016514778137 average time 0.00916537127198332\n",
            "loss 0.2827550768852234 average time 0.009164452496212713\n",
            "loss 0.25292789936065674 average time 0.00916276001545901\n",
            "loss 0.31687745451927185 average time 0.009162058409891144\n",
            "loss 0.3215210437774658 average time 0.00915998570533601\n",
            "loss 0.30836817622184753 average time 0.009158552845056964\n",
            "loss 0.3026335537433624 average time 0.009156351821741777\n",
            "loss 0.3040682375431061 average time 0.009155152230970255\n",
            "loss 0.3106659948825836 average time 0.0091538098554276\n",
            "loss 0.2857823669910431 average time 0.00915290603505472\n",
            "loss 0.30867695808410645 average time 0.00915322177760705\n",
            "loss 0.3280481994152069 average time 0.009151555108250643\n",
            "loss 0.2816838026046753 average time 0.009151001613371016\n",
            "loss 0.2603479027748108 average time 0.009150020943133396\n",
            "loss 0.27756398916244507 average time 0.009148266797102587\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0\n",
            "loss 0.922260046005249 average time 0.013177456309949775\n",
            "loss 0.5773457884788513 average time 0.011111219429994889\n",
            "loss 0.41478532552719116 average time 0.01041175185663936\n",
            "loss 0.4466990828514099 average time 0.010073354319993087\n",
            "loss 0.4945479929447174 average time 0.009858512365990463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/asian_barrier_option/distributed_train.py', 'w') as f:\n",
        "    f.write('''\n",
        "import cupy\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.dlpack import from_dlpack\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from apex import amp\n",
        "from ignite.engine import Engine, Events\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import Adam\n",
        "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
        "from ignite.handlers import ModelCheckpoint\n",
        "from apex.parallel import DistributedDataParallel\n",
        "import argparse\n",
        "from model import Net\n",
        "from cupy_dataset import OptionDataSet\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser = argparse.ArgumentParser()\n",
        "# this local_rank arg is automaticall set by distributed launch\n",
        "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.distributed = False\n",
        "if 'WORLD_SIZE' in os.environ:\n",
        "    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
        "\n",
        "if args.distributed:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl',\n",
        "                                         init_method='env://')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "model = Net().cuda()\n",
        "loss_fn = MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "opt_level = 'O1'\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
        "if args.distributed:\n",
        "    model = DistributedDataParallel(model)\n",
        "dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=10240, seed=args.local_rank)\n",
        "\n",
        "def train_update(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x = batch[0]\n",
        "    y = batch[1]\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred[:,0], y)\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_update)\n",
        "log_interval = 100\n",
        "\n",
        "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
        "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
        "\n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
        "    if iter % log_interval == 0:\n",
        "        print('loss', engine.state.output)\n",
        "\n",
        "trainer.run(dataset, max_epochs=100)\n",
        "''')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3pYaxNetDfsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "a2HhbMbUDypw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone Apex\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "\n",
        "# Set CUDA home explicitly and try installation\n",
        "!CUDA_HOME=/usr/local/cuda-12.2 TORCH_CUDA_ARCH_LIST=\"8.0;8.6;8.9;9.0\" pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "metadata": {
        "id": "u-H7vMeCNFA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node=4 /content/drive/MyDrive/asian_barrier_option/distributed_train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBest4txLQXT",
        "outputId": "355ebaee-53e6-43b4-cbe2-f6190949cb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  main()\n",
            "W0111 02:45:41.069000 17689 torch/distributed/run.py:793] \n",
            "W0111 02:45:41.069000 17689 torch/distributed/run.py:793] *****************************************\n",
            "W0111 02:45:41.069000 17689 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0111 02:45:41.069000 17689 torch/distributed/run.py:793] *****************************************\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/asian_barrier_option/distributed_train.py\", line 12, in <module>\n",
            "    from apex import amp\n",
            "ModuleNotFoundError: No module named 'apex'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/asian_barrier_option/distributed_train.py\", line 12, in <module>\n",
            "    from apex import amp\n",
            "ModuleNotFoundError: No module named 'apex'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/asian_barrier_option/distributed_train.py\", line 12, in <module>\n",
            "    from apex import amp\n",
            "ModuleNotFoundError: No module named 'apex'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/asian_barrier_option/distributed_train.py\", line 12, in <module>\n",
            "    from apex import amp\n",
            "ModuleNotFoundError: No module named 'apex'\n",
            "W0111 02:45:48.547000 17689 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 17701 closing signal SIGTERM\n",
            "W0111 02:45:48.548000 17689 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 17702 closing signal SIGTERM\n",
            "W0111 02:45:48.548000 17689 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 17703 closing signal SIGTERM\n",
            "E0111 02:45:48.620000 17689 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 17704) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 2853, in wrapper\n",
            "    return arg(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 204, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py\", line 189, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/content/drive/MyDrive/asian_barrier_option/distributed_train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-01-11_02:45:48\n",
            "  host      : a5d76b7ce359\n",
            "  rank      : 3 (local_rank: 3)\n",
            "  exitcode  : 1 (pid: 17704)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbrjrlIpLVen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}